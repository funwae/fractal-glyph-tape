# Fractal Glyph Tape (FGT)

> A fractal-addressable phrase memory that makes language **denser**, **more cross-lingual**, and **more explorable** for LLMs.

[![License: Proprietary](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Status: Research Prototype](https://img.shields.io/badge/status-research%20prototype-orange.svg)]()

Fractal Glyph Tape (FGT) is a research prototype from **Glyphd Labs** that adds a new layer beneath large language models:

- It clusters similar sentences and phrases into **phrase families**.
- It assigns each family a compact **glyph ID** built from Mandarin characters used purely as **glyphs** (no native semantics).
- It places these glyph IDs onto a **fractal tape**â€”a recursive triangular address space that preserves semantic neighborhoods.

Text can then be represented as a **hybrid** of:

- normal tokens, and
- **glyph tokens** that act as pointers into a shared phrase memory.

LLMs can be trained or adapted to **read and write this inner code**, enabling:

- ğŸ“¦ **Semantic compression** â€“ smaller corpora and logs with reconstructable meaning
- ğŸ§  **Effective context extension** â€“ more usable signal per token under fixed context windows
- ğŸŒ **Cross-lingual bridging** â€“ shared glyph IDs for phrase families spanning multiple languages
- ğŸ” **Explorable phrase space** â€“ an interactive fractal map of "things we say"

## ğŸ¯ What is Fractal Glyph Tape?

Fractal Glyph Tape (FGT) is a revolutionary approach to language representation that solves three fundamental problems:

### 1. Redundant Storage
Modern systems store "Can you send me that file?" millions of times across corpora. **FGT stores it once as a glyph**.

### 2. Token Limitations
LLMs are bottlenecked by token-based context windows. **FGT extends effective context by 25-50%** through semantic compression.

### 3. Language Barriers
No system connects "send me the file" (EN) with "envÃ­ame el archivo" (ES) and "æŠŠæ–‡ä»¶å‘ç»™æˆ‘" (ZH). **FGT maps all three to the same glyph**.

## ğŸš€ Core Innovation

```
English:  "Can you send me that file?"     â”€â”€â”
Spanish:  "Â¿Puedes enviarme ese archivo?"  â”€â”€â”œâ”€â†’ Glyph: è°·é˜œ
Chinese:  "ä½ èƒ½æŠŠé‚£ä¸ªæ–‡ä»¶å‘ç»™æˆ‘å—ï¼Ÿ"           â”€â”€â”˜
          â†“
    Fractal Address: L-R-C-L
    Context: File sharing request
    Languages: EN, ES, ZH
    Compression: 70% (3 chars vs ~40 chars avg)
```

**Key Insight**: Mandarin characters are used as **pure visual symbols** (not for linguistic meaning), providing a dense alphabet for encoding phrase families.

## What's in this repo?

- âœ… **Ingestion pipeline** â€“ from raw text to structured phrases
- âœ… **Multilingual embeddings & clustering** â€“ phrase families with rich metadata
- âœ… **Glyph encoding system** â€“ integer glyph IDs â†’ Mandarin glyph strings
- âœ… **Fractal tape builder** â€“ 2D projection + recursive triangular addressing
- âœ… **Hybrid tokenizer** â€“ wraps a base tokenizer with glyph-aware spans
- âœ… **LLM adapter** â€“ training & inference helpers for glyph-aware models
- âœ… **Visualization API** â€“ backend for a fractal phrase-map web UI
- âœ… **Experiment scripts** â€“ compression, context, and cross-lingual evaluations

See [`docs/README.md`](docs/README.md) for the full technical spec.

## âœ¨ Features

### Compression & Efficiency
- âœ… **50-80% corpus compression** while preserving semantics
- âœ… **15-30% token reduction** for LLM processing
- âœ… **+25-50% context capacity** in same token budget
- âœ… **Training acceleration** through structured representations

### Cross-Lingual Capabilities
- ğŸŒ **Language-agnostic phrase indexing**
- ğŸ”„ **Translation-free multilingual search**
- ğŸŒ **Cross-lingual semantic bridging** (ENâ†”ESâ†”ZH)
- ğŸ“Š **90-95% cross-lingual retrieval precision**

### Visualization & Tools
- ğŸ—ºï¸ **Interactive fractal map** of semantic space
- ğŸ“Š **Publication-quality analytics** and plots
- ğŸ”§ **Hybrid tokenizer** for LLM integration
- ğŸ¯ **RESTful API** for tape querying

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/funwae/fractal-glyph-tape.git
cd fractal-glyph-tape

# Install dependencies
pip install -r requirements.txt

# Install package
pip install -e .
```

## âš¡ Quick Start

### Build Your First Tape (Phase 0)

```bash
# Use demo configuration (100k phrases, 10k clusters)
fgt build --config configs/demo.yaml

# Or use test configuration (100 phrases, 20 clusters)
fgt build --config configs/test.yaml
```

This will:
1. Ingest phrases from `data/raw/`
2. Embed phrases using SentenceTransformers
3. Cluster into semantic families
4. Assign glyph IDs (Mandarin characters)
5. Build fractal tape with 2D projection
6. Create SQLite database for querying

### Explore the Visualization (Phase 1)

```bash
# Start visualization server
python scripts/run_viz_server.py --tape tape/v1/tape_index.db

# Open browser to http://localhost:8000/viz
# - See interactive fractal map
# - Click clusters to see examples
# - Hover for glyph details
```

### Test Compression (Phase 1)

```bash
# Run compression experiments
python scripts/compression_experiments.py \
    --phrases data/phrases.jsonl \
    --tape tape/v1/tape_index.db \
    --sample-size 1000

# View results in results/compression_experiments/
```

### LLM Integration (Phase 2)

```bash
# Fine-tune a glyph-aware language model
python scripts/finetune_glyph_model.py \
    --model gpt2 \
    --phrases data/phrases.jsonl \
    --tape tape/v1/tape_index.db \
    --epochs 3 \
    --max-samples 10000

# Test context efficiency
python scripts/context_efficiency_experiments.py \
    --phrases data/phrases.jsonl \
    --tape tape/v1/tape_index.db
```

### Cross-Lingual Demo (Phase 3)

```bash
# Build multilingual tape (EN/ES/ZH)
python scripts/run_full_build.py --config configs/multilingual.yaml

# Run cross-lingual experiments
python scripts/cross_lingual_experiments.py \
    --phrases data/multilingual_phrases.jsonl \
    --tape tape/multilingual_v1/tape_index.db

# View results showing same glyph for equivalent phrases across languages!
```

## ğŸ“ˆ Results â€” Smart Memory vs Naive Truncation

We benchmarked the Fractal Glyph Memory system on synthetic multi-turn dialogs designed to mimic a real agent:

- Conversations contain **early setup information** (goals, constraints, preferences),
- followed by **filler turns and topic drift**,
- and end with a **question** that depends on that buried early information.

We compared two context strategies under fixed token budgets:

1. **RAW-TRUNCATE**
   Take the last N tokens of the conversation and feed them to the model.

2. **FGT-FOVEATED (Fractal Glyph Tape)**
   Simulate the behavior of the Fractal Glyph Memory Service:
   - Allocate ~30% of the budget to **very early turns** (where setup lives),
   - ~30% to **semantically relevant turns** (keyword/embedding-matched to the question),
   - ~40% to **recent turns** for conversational coherence.

### ğŸ¯ Accuracy vs Token Budget

On a synthetic benchmark (150 episodes, train/val/test), we measured answer accuracy:

| Token Budget | RAW-TRUNCATE | FGT-FOVEATED | Improvement |
|--------------|--------------|--------------|-------------|
| **256**      | 26.7%        | **73.3%**    | **+46.7 pp** |
| 512          | 73.3%        | 73.3%        | +0.0 pp      |
| 1024         | 73.3%        | 73.3%        | +0.0 pp      |
| 2048         | 73.3%        | 73.3%        | +0.0 pp      |

**Key insight:**

- Under **tight budgets** (256 tokens), naive truncation forgets the early setup almost every time.
  Fractal Glyph memory's foveated policy pulls in **early + relevant + recent** slices and nearly *triples* accuracy.
- Once the token budget is big enough to include the whole episode (512+ in this setup), both strategies converge, as expected.

In other words:

> Fractal Glyph Memory is not a "bigger context window."
> It's a **smarter way to pack the right context** into the same number of tokens.

Full methodology and benchmark details: [`docs/PHASE-5-RESULTS.md`](docs/PHASE-5-RESULTS.md).

## ğŸ”§ Usage Examples

### Encode Text to Glyphs

```python
from tokenizer import HybridTokenizer

# Initialize hybrid tokenizer
tokenizer = HybridTokenizer(
    base_tokenizer="gpt2",
    tape_db_path="tape/v1/tape_index.db",
    similarity_threshold=0.75
)

# Encode text
result = tokenizer.encode_hybrid(
    "Can you send me that file?",
    return_details=True
)

print(f"Glyph: {result['encoding_decisions'][0]['glyph']}")
print(f"Token reduction: {result['glyph_encoded']} phrases as glyphs")
```

### Query the Tape

```python
from tape import TapeStorage

# Open tape database
with TapeStorage("tape/v1/tape_index.db") as storage:
    storage.connect()

    # Get cluster details
    cluster = storage.get_cluster_by_glyph("è°·é˜œ")

    print(f"Cluster size: {cluster['size']} phrases")
    print(f"Fractal address: {cluster['fractal_address']}")
    print(f"Examples:")
    for ex in cluster['metadata']['examples'][:5]:
        print(f"  - {ex['text']}")
```

### Cross-Lingual Search

```python
# Query in English
query = "Can you send me that file?"
glyph = encode_to_glyph(query)  # â†’ è°·é˜œ

# Retrieve in Spanish
spanish_results = get_cluster_phrases(glyph, lang="es")
# â†’ "Â¿Puedes enviarme ese archivo?"

# Retrieve in Chinese
chinese_results = get_cluster_phrases(glyph, lang="zh")
# â†’ "ä½ èƒ½æŠŠé‚£ä¸ªæ–‡ä»¶å‘ç»™æˆ‘å—ï¼Ÿ"
```

## ğŸ“Š Performance

### Compression Results
| Method | Compression Ratio | Bytes/Phrase | Notes |
|--------|------------------|--------------|-------|
| Raw Text | 1.0x | 45.2 | Baseline |
| Gzip | 2.8x | 16.1 | Good general compression |
| BPE (GPT-2) | 0.9x | 50.3 | Larger due to vocab |
| **FGT Glyphs** | **2.1x** | **21.5** | **+ Semantic preservation** |

### Context Efficiency
| Encoding | Tokens/Doc | Docs/Window (2048) | Improvement |
|----------|------------|-------------------|-------------|
| Regular | 185.4 | 11.0 | Baseline |
| **Hybrid** | **142.7** | **14.3** | **+30%** |

### Cross-Lingual Retrieval
| Language Pair | Precision@1 | Precision@5 |
|---------------|-------------|-------------|
| EN â†’ ES | 94% | 98% |
| EN â†’ ZH | 91% | 96% |
| ES â†’ ZH | 89% | 95% |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Text Corpus (English, Spanish, Chinese, ...)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phrase Extraction & Language Detection             â”‚
â”‚ â†’ Segmentation, filtering, deduplication           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multilingual Embedding                              â”‚
â”‚ â†’ SentenceTransformer (paraphrase-multilingual)    â”‚
â”‚ â†’ 384-dim vectors, language-agnostic               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Clustering                                 â”‚
â”‚ â†’ MiniBatchKMeans (10k clusters)                   â”‚
â”‚ â†’ Phrases cluster by meaning, not language         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Glyph Assignment                                    â”‚
â”‚ â†’ Mandarin characters as base-N IDs                â”‚
â”‚ â†’ cluster_id â†’ glyph_string (è°·é˜œ, é˜œè°·, ...)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fractal Tape Construction                           â”‚
â”‚ â†’ UMAP 2D projection                               â”‚
â”‚ â†’ Triangular fractal addressing (L-R-C-T)          â”‚
â”‚ â†’ SQLite storage with indexes                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Applications                                        â”‚
â”‚ â”œâ”€ Hybrid Tokenization for LLMs                    â”‚
â”‚ â”œâ”€ Cross-Lingual Retrieval                         â”‚
â”‚ â”œâ”€ Interactive Visualization                       â”‚
â”‚ â””â”€ Compression & Context Extension                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Documentation

### Getting Started
- [Quick Start Guide](QUICKSTART.md) - Get up and running in 5 minutes
- [Installation Guide](docs/INSTALLATION.md) - Detailed setup instructions
- [Configuration Guide](docs/CONFIGURATION.md) - Customize your build

### Phase Documentation
- **[Phase 0: Core System](claude.md)** - Build instructions and architecture
- **[Phase 1: Visualization](docs/PHASE1_VISUALIZATION.md)** - API, plots, experiments
- **[Phase 2: LLM Integration](docs/PHASE2_LLM_INTEGRATION.md)** - Tokenization, fine-tuning
- **[Phase 3: Cross-Lingual](docs/PHASE3_CROSS_LINGUAL.md)** - Multilingual bridging

### Technical Details
- [System Architecture](docs/30-system-architecture-overview.md)
- [Fractal Addressing](docs/20-fractal-addressing-spec.md)
- [Clustering Math](docs/22-phrase-clustering-math.md)
- [Data Pipeline](docs/31-data-pipeline-design.md)

### Experiments
- [Compression Experiments](docs/61-corpus-compression-experiments.md)
- [Context Efficiency](docs/62-context-window-efficiency-experiments.md)
- [Multilingual Bridge](docs/63-multilingual-bridge-experiments.md)

## ğŸ¯ Use Cases

### 1. Long-Context LLM Applications
**Problem**: GPT models limited to 4k-8k tokens
**Solution**: Hybrid encoding fits 30% more semantic content in same budget

### 2. Multilingual Customer Support
**Problem**: Support queries in 50+ languages
**Solution**: All equivalent queries map to same glyph â†’ unified routing

### 3. International Documentation
**Problem**: Maintain parallel docs in multiple languages
**Solution**: Glyph-based semantic index works across all languages

### 4. Efficient Model Training
**Problem**: Training on large corpora is expensive
**Solution**: 20-30% fewer tokens â†’ faster epochs, lower costs

### 5. Cross-Lingual Search
**Problem**: Users search in different languages for same content
**Solution**: Language-agnostic glyph search returns unified results

## ğŸ›£ï¸ Roadmap

### âœ… Completed (Phases 0-5)
- [x] Core phrase clustering and glyph assignment
- [x] Fractal tape construction with 2D projection
- [x] Interactive web visualization
- [x] Compression experiments vs baselines
- [x] Hybrid tokenizer for LLMs
- [x] Fine-tuning pipeline
- [x] Context efficiency experiments
- [x] Multilingual corpus (EN/ES/ZH)
- [x] Cross-lingual retrieval experiments
- [x] Language detection and analysis
- [x] Phase 5: Fractal Glyph Memory Service with foveated context policy

### ğŸ”œ Next Steps
- [ ] Scale to 1M+ phrases, 50k+ clusters
- [ ] Add 10+ more languages (FR, DE, JA, AR, ...)
- [ ] Domain-specific tapes (medical, legal, technical)
- [ ] Production API deployment
- [ ] Real-time online updates
- [ ] Few-shot learning experiments
- [ ] Research paper publication

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas for contribution**:
- Adding new languages to multilingual corpus
- Domain-specific phrase collections
- Visualization improvements
- Performance optimizations
- Documentation and tutorials
- Benchmark datasets

## ğŸ“œ License

This project is licensed under a Proprietary License - see the [LICENSE](LICENSE) file for details. **Non-commercial use only. Commercial use requires explicit written permission.**

## ğŸ† Citation

If you use Fractal Glyph Tape in your research, please cite:

```bibtex
@software{fractal_glyph_tape_2025,
  title = {Fractal Glyph Tape: Semantic Compression and Cross-Lingual Bridging},
  author = {Glyphd Labs},
  year = {2025},
  url = {https://github.com/funwae/fractal-glyph-tape},
  note = {Research prototype for phrase-level semantic compression}
}
```

## ğŸ™ Acknowledgments

- **SentenceTransformers** for multilingual embeddings
- **Hugging Face** for transformer models
- **scikit-learn** for clustering algorithms
- **UMAP** for dimensionality reduction
- **FastAPI** for visualization backend

## ğŸ“ Contact

- **Website**: [glyphd.com](https://glyphd.com)
- **Issues**: [GitHub Issues](https://github.com/funwae/fractal-glyph-tape/issues)
- **Email**: info@glyphd.com

## ğŸŒŸ Star History

If you find this project useful, please consider giving it a star! â­

---

**Built with â¤ï¸ by Glyphd Labs**

*Bridging languages, compressing semantics, extending context.*
